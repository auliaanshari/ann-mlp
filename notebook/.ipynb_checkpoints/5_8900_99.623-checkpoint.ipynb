{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "nuhQNHqS_fMn"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "id": "PWOkvP6F_fM0",
    "outputId": "216de9e8-74fa-479b-81f6-5d07370d1805"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>T</th>\n",
       "      <th>B</th>\n",
       "      <th>SU1</th>\n",
       "      <th>SU2</th>\n",
       "      <th>RT1</th>\n",
       "      <th>RT2</th>\n",
       "      <th>RT3</th>\n",
       "      <th>RT4</th>\n",
       "      <th>I1</th>\n",
       "      <th>I2</th>\n",
       "      <th>N1</th>\n",
       "      <th>N2</th>\n",
       "      <th>N3</th>\n",
       "      <th>J</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017</td>\n",
       "      <td>Januari</td>\n",
       "      <td>14835</td>\n",
       "      <td>18172</td>\n",
       "      <td>14183</td>\n",
       "      <td>24728</td>\n",
       "      <td>841756</td>\n",
       "      <td>201048</td>\n",
       "      <td>46800</td>\n",
       "      <td>39938</td>\n",
       "      <td>56324</td>\n",
       "      <td>32492</td>\n",
       "      <td>5994</td>\n",
       "      <td>1296270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017</td>\n",
       "      <td>Februari</td>\n",
       "      <td>14893</td>\n",
       "      <td>19026</td>\n",
       "      <td>14371</td>\n",
       "      <td>24394</td>\n",
       "      <td>832284</td>\n",
       "      <td>200774</td>\n",
       "      <td>47536</td>\n",
       "      <td>37282</td>\n",
       "      <td>57218</td>\n",
       "      <td>31412</td>\n",
       "      <td>5329</td>\n",
       "      <td>1284519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017</td>\n",
       "      <td>Maret</td>\n",
       "      <td>13676</td>\n",
       "      <td>17706</td>\n",
       "      <td>13864</td>\n",
       "      <td>23071</td>\n",
       "      <td>789354</td>\n",
       "      <td>197318</td>\n",
       "      <td>41005</td>\n",
       "      <td>35755</td>\n",
       "      <td>55068</td>\n",
       "      <td>30468</td>\n",
       "      <td>5737</td>\n",
       "      <td>1223022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017</td>\n",
       "      <td>April</td>\n",
       "      <td>15780</td>\n",
       "      <td>20222</td>\n",
       "      <td>15791</td>\n",
       "      <td>26153</td>\n",
       "      <td>893393</td>\n",
       "      <td>224847</td>\n",
       "      <td>47793</td>\n",
       "      <td>38759</td>\n",
       "      <td>63590</td>\n",
       "      <td>34198</td>\n",
       "      <td>6926</td>\n",
       "      <td>1387452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017</td>\n",
       "      <td>Mei</td>\n",
       "      <td>15698</td>\n",
       "      <td>17508</td>\n",
       "      <td>14863</td>\n",
       "      <td>24681</td>\n",
       "      <td>851205</td>\n",
       "      <td>222635</td>\n",
       "      <td>45794</td>\n",
       "      <td>37655</td>\n",
       "      <td>60998</td>\n",
       "      <td>26452</td>\n",
       "      <td>5915</td>\n",
       "      <td>1323404</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      T         B    SU1    SU2    RT1    RT2     RT3     RT4     I1     I2  \\\n",
       "0  2017   Januari  14835  18172  14183  24728  841756  201048  46800  39938   \n",
       "1  2017  Februari  14893  19026  14371  24394  832284  200774  47536  37282   \n",
       "2  2017     Maret  13676  17706  13864  23071  789354  197318  41005  35755   \n",
       "3  2017     April  15780  20222  15791  26153  893393  224847  47793  38759   \n",
       "4  2017       Mei  15698  17508  14863  24681  851205  222635  45794  37655   \n",
       "\n",
       "      N1     N2    N3        J  \n",
       "0  56324  32492  5994  1296270  \n",
       "1  57218  31412  5329  1284519  \n",
       "2  55068  30468  5737  1223022  \n",
       "3  63590  34198  6926  1387452  \n",
       "4  60998  26452  5915  1323404  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data Train Load dataset\n",
    "datatrain = pd.read_csv(\"/home/systemcommand/anggy/app/data/trainanggy.csv\", delimiter=',')\n",
    "datatrain.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_g9T_SEU_fM2",
    "outputId": "75d9f810-842f-4476-f80d-e392b6b1d165"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 24 entries, 0 to 23\n",
      "Data columns (total 14 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   T       24 non-null     int64 \n",
      " 1   B       24 non-null     object\n",
      " 2   SU1     24 non-null     int64 \n",
      " 3   SU2     24 non-null     int64 \n",
      " 4   RT1     24 non-null     int64 \n",
      " 5   RT2     24 non-null     int64 \n",
      " 6   RT3     24 non-null     int64 \n",
      " 7   RT4     24 non-null     int64 \n",
      " 8   I1      24 non-null     int64 \n",
      " 9   I2      24 non-null     int64 \n",
      " 10  N1      24 non-null     int64 \n",
      " 11  N2      24 non-null     int64 \n",
      " 12  N3      24 non-null     int64 \n",
      " 13  J       24 non-null     int64 \n",
      "dtypes: int64(13), object(1)\n",
      "memory usage: 2.8+ KB\n"
     ]
    }
   ],
   "source": [
    "datatrain.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 304
    },
    "id": "lXyRijjI_fM3",
    "outputId": "be415e09-64e4-40d1-b573-63584d022939"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>T</th>\n",
       "      <th>SU1</th>\n",
       "      <th>SU2</th>\n",
       "      <th>RT1</th>\n",
       "      <th>RT2</th>\n",
       "      <th>RT3</th>\n",
       "      <th>RT4</th>\n",
       "      <th>I1</th>\n",
       "      <th>I2</th>\n",
       "      <th>N1</th>\n",
       "      <th>N2</th>\n",
       "      <th>N3</th>\n",
       "      <th>J</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>24.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>24.00000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>2.400000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2017.500000</td>\n",
       "      <td>16104.916667</td>\n",
       "      <td>18290.208333</td>\n",
       "      <td>16534.291667</td>\n",
       "      <td>24995.458333</td>\n",
       "      <td>852528.62500</td>\n",
       "      <td>250181.791667</td>\n",
       "      <td>49658.750000</td>\n",
       "      <td>41513.708333</td>\n",
       "      <td>62788.500000</td>\n",
       "      <td>31665.083333</td>\n",
       "      <td>6312.791667</td>\n",
       "      <td>1.370574e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.510754</td>\n",
       "      <td>1264.363525</td>\n",
       "      <td>1241.900471</td>\n",
       "      <td>1881.335453</td>\n",
       "      <td>1115.935170</td>\n",
       "      <td>33973.08554</td>\n",
       "      <td>29215.222834</td>\n",
       "      <td>3421.754727</td>\n",
       "      <td>4409.177016</td>\n",
       "      <td>4970.613609</td>\n",
       "      <td>3459.028827</td>\n",
       "      <td>585.891144</td>\n",
       "      <td>7.163533e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2017.000000</td>\n",
       "      <td>13676.000000</td>\n",
       "      <td>15291.000000</td>\n",
       "      <td>13584.000000</td>\n",
       "      <td>22892.000000</td>\n",
       "      <td>774140.00000</td>\n",
       "      <td>197318.000000</td>\n",
       "      <td>41005.000000</td>\n",
       "      <td>33836.000000</td>\n",
       "      <td>54898.000000</td>\n",
       "      <td>23525.000000</td>\n",
       "      <td>5329.000000</td>\n",
       "      <td>1.223022e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2017.000000</td>\n",
       "      <td>15246.500000</td>\n",
       "      <td>17696.500000</td>\n",
       "      <td>15420.250000</td>\n",
       "      <td>24221.500000</td>\n",
       "      <td>839388.00000</td>\n",
       "      <td>229978.750000</td>\n",
       "      <td>47423.000000</td>\n",
       "      <td>38483.000000</td>\n",
       "      <td>59764.000000</td>\n",
       "      <td>29437.000000</td>\n",
       "      <td>5797.750000</td>\n",
       "      <td>1.321748e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2017.500000</td>\n",
       "      <td>15957.000000</td>\n",
       "      <td>18305.500000</td>\n",
       "      <td>16122.000000</td>\n",
       "      <td>24905.500000</td>\n",
       "      <td>854607.50000</td>\n",
       "      <td>255383.500000</td>\n",
       "      <td>49450.000000</td>\n",
       "      <td>40874.500000</td>\n",
       "      <td>63619.500000</td>\n",
       "      <td>32318.500000</td>\n",
       "      <td>6184.500000</td>\n",
       "      <td>1.379902e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2018.000000</td>\n",
       "      <td>17105.250000</td>\n",
       "      <td>19032.500000</td>\n",
       "      <td>17938.000000</td>\n",
       "      <td>25862.250000</td>\n",
       "      <td>875528.50000</td>\n",
       "      <td>271391.000000</td>\n",
       "      <td>52196.750000</td>\n",
       "      <td>44572.750000</td>\n",
       "      <td>65514.750000</td>\n",
       "      <td>33680.000000</td>\n",
       "      <td>6865.000000</td>\n",
       "      <td>1.418112e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2018.000000</td>\n",
       "      <td>18750.000000</td>\n",
       "      <td>20804.000000</td>\n",
       "      <td>20184.000000</td>\n",
       "      <td>27067.000000</td>\n",
       "      <td>911332.00000</td>\n",
       "      <td>295305.000000</td>\n",
       "      <td>56538.000000</td>\n",
       "      <td>49724.000000</td>\n",
       "      <td>74471.000000</td>\n",
       "      <td>37372.000000</td>\n",
       "      <td>7422.000000</td>\n",
       "      <td>1.482129e+06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 T           SU1           SU2           RT1           RT2  \\\n",
       "count    24.000000     24.000000     24.000000     24.000000     24.000000   \n",
       "mean   2017.500000  16104.916667  18290.208333  16534.291667  24995.458333   \n",
       "std       0.510754   1264.363525   1241.900471   1881.335453   1115.935170   \n",
       "min    2017.000000  13676.000000  15291.000000  13584.000000  22892.000000   \n",
       "25%    2017.000000  15246.500000  17696.500000  15420.250000  24221.500000   \n",
       "50%    2017.500000  15957.000000  18305.500000  16122.000000  24905.500000   \n",
       "75%    2018.000000  17105.250000  19032.500000  17938.000000  25862.250000   \n",
       "max    2018.000000  18750.000000  20804.000000  20184.000000  27067.000000   \n",
       "\n",
       "                RT3            RT4            I1            I2            N1  \\\n",
       "count      24.00000      24.000000     24.000000     24.000000     24.000000   \n",
       "mean   852528.62500  250181.791667  49658.750000  41513.708333  62788.500000   \n",
       "std     33973.08554   29215.222834   3421.754727   4409.177016   4970.613609   \n",
       "min    774140.00000  197318.000000  41005.000000  33836.000000  54898.000000   \n",
       "25%    839388.00000  229978.750000  47423.000000  38483.000000  59764.000000   \n",
       "50%    854607.50000  255383.500000  49450.000000  40874.500000  63619.500000   \n",
       "75%    875528.50000  271391.000000  52196.750000  44572.750000  65514.750000   \n",
       "max    911332.00000  295305.000000  56538.000000  49724.000000  74471.000000   \n",
       "\n",
       "                 N2           N3             J  \n",
       "count     24.000000    24.000000  2.400000e+01  \n",
       "mean   31665.083333  6312.791667  1.370574e+06  \n",
       "std     3459.028827   585.891144  7.163533e+04  \n",
       "min    23525.000000  5329.000000  1.223022e+06  \n",
       "25%    29437.000000  5797.750000  1.321748e+06  \n",
       "50%    32318.500000  6184.500000  1.379902e+06  \n",
       "75%    33680.000000  6865.000000  1.418112e+06  \n",
       "max    37372.000000  7422.000000  1.482129e+06  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# EDA/ data description\n",
    "this = datatrain.copy()\n",
    "this.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "BW3EMuNm_fM3"
   },
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "train = this.drop(['T','B'],axis=1)\n",
    "\n",
    "# Preprocessing/ transform(normalizing)\n",
    "scaler = preprocessing.MinMaxScaler().fit(train)\n",
    "data_train = scaler.transform(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hcKi8waW_fM4",
    "outputId": "3b78b2df-e38b-439d-a000-afaad29ddb11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24, 11)\n"
     ]
    }
   ],
   "source": [
    "# Selecting Variables\n",
    "datax = []\n",
    "for i in range(len(data_train)):\n",
    "    tes = data_train[i][:11]\n",
    "    datax.append(tes)\n",
    "\n",
    "datay = []\n",
    "for i in range(len(data_train)):\n",
    "    tes = data_train[i][-1]\n",
    "    datay.append(tes)\n",
    "\n",
    "x = np.array(datax,np.float32)\n",
    "y = np.array(datay,np.float32)\n",
    "\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "lAh2tA-R_fM5"
   },
   "outputs": [],
   "source": [
    "# Data Test Load dataset\n",
    "datatest = pd.read_csv(\"/home/systemcommand/anggy/app/data/testanggy.csv\", delimiter=',')\n",
    "test = datatest.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "cH28mHS3_fM6"
   },
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "testing = test.drop(['T','B'],axis=1)\n",
    "\n",
    "# Preprocessing/ Transform(normalizing)\n",
    "scaler = preprocessing.MinMaxScaler().fit(testing)\n",
    "data_test = scaler.transform(testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "-3UfslHt_fM7"
   },
   "outputs": [],
   "source": [
    "# Selecting Variables\n",
    "\n",
    "datax1 = []\n",
    "for i in range(len(data_test)):\n",
    "    tes = data_test[i][:11]\n",
    "    datax1.append(tes)\n",
    "\n",
    "datay1 = []\n",
    "for i in range(len(data_test)):\n",
    "    tes = data_test[i][-1]\n",
    "    datay1.append(tes)\n",
    "\n",
    "xx = np.array(datax1,np.float32)\n",
    "yy = np.array(datay1,np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Nlz-v_OZ_fM8"
   },
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "n = x.shape[1]\n",
    "m = 5\n",
    "M = 1\n",
    "init_arr_item = 0.0\n",
    "\n",
    "learning_rate = 0.001\n",
    "TRAIN_ITER = 8900"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Rkx_Vmyc_fM9"
   },
   "outputs": [],
   "source": [
    "# Make Placeholder \n",
    "w = np.ones([m,n], np.float32)*init_arr_item\n",
    "v = np.ones([M,m], np.float32)*init_arr_item\n",
    "b_j = np.zeros([m], np.float32)*init_arr_item\n",
    "b_k = np.zeros([M], np.float32)*init_arr_item\n",
    "\n",
    "z_pred = np.zeros([y.shape[0], m], np.float32)\n",
    "y_pred = np.zeros([y.shape[0], M], np.float32)\n",
    "E = np.zeros(M, np.float32)\n",
    "\n",
    "v_new = np.zeros([M,m], np.float32)\n",
    "delta_k = np.zeros(M, np.float32)\n",
    "\n",
    "w_new = np.zeros([m,n], np.float32)\n",
    "delta_j = np.zeros(m, np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "BxMdoD83_fM9"
   },
   "outputs": [],
   "source": [
    "# Define Sigmoid activation function\n",
    "def f_sigmoid(value):\n",
    "    return 1.0/(1.0+np.exp(-value))\n",
    "\n",
    "def f_sigmoid_derivation(value):\n",
    "    return f_sigmoid(value)*(1-f_sigmoid(value))\n",
    "  \n",
    "# Define Linear activation function\n",
    "def f_linear(value):\n",
    "    return value\n",
    "\n",
    "def f_linear_derivation(value):\n",
    "    return np.ones(value.shape, np.float32)\n",
    "  \n",
    "# Hidden Layer activation function wrapper\n",
    "def f_activation(value):\n",
    "    return f_sigmoid(value)\n",
    "  \n",
    "def f_activation_derivation(value):\n",
    "    return f_sigmoid_derivation(value)\n",
    "\n",
    "# Output Layer activation function wrapper\n",
    "def f_activation_output(value):\n",
    "    return f_linear(value)\n",
    "\n",
    "def f_activation_derivation_output(value):\n",
    "    return f_linear_derivation(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "oWieVuy1_fM-"
   },
   "outputs": [],
   "source": [
    "# Feed Forward\n",
    "def calc_z_j(x_train, w_train, b_j_train):\n",
    "    for j in range(m):\n",
    "        z_pred[:,j] = np.dot(x_train,w_train[j,:]) + b_j_train[j]\n",
    "    \n",
    "    return z_pred\n",
    "    \n",
    "def calc_y_k(z_train, v_train, b_k_train):\n",
    "    for k in range(M):\n",
    "        y_pred[:,k] = np.dot(z_train, v_train[k,:]) + b_k_train[k]\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "FLSx3bXg_fM_"
   },
   "outputs": [],
   "source": [
    "# Calculate Loss\n",
    "def error_calc(target, output):\n",
    "    for k in range(M):\n",
    "        E[k] = 0.5*(target[k] - output[k])**2\n",
    "    return np.sum(E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "ALOQ5-Ct_fM_"
   },
   "outputs": [],
   "source": [
    "# Backpropagation\n",
    "def calc_v_new(v_last, y, y_prediction, z_prediction):\n",
    "    delta_k = np.transpose(f_activation_derivation_output(y_prediction)*(np.reshape(y, (y.shape[0], 1)) - y_prediction))\n",
    "        \n",
    "    for j in range(m):\n",
    "        v_new[:,j] = v_last[:,j] + learning_rate*np.dot(delta_k,z_prediction[:,j])\n",
    "            \n",
    "    return v_new, delta_k\n",
    "\n",
    "\n",
    "def calc_w_new(w_last, v_new, delta_k, z_prediction, x_input):\n",
    "    sum_v = np.dot(np.transpose(v_new), delta_k)\n",
    "    \n",
    "    delta_j = np.transpose(f_activation_derivation(z_prediction))*sum_v\n",
    "    \n",
    "    for i in range(n):\n",
    "            w_new[:,i] = w_last[:,i] + learning_rate*np.dot(delta_j,x_input[:,i])\n",
    "            \n",
    "    return w_new, delta_j\n",
    "    \n",
    "def calc_b_k_new(delta_k, b_k_last):\n",
    "    return b_k_last +  learning_rate*np.sum(delta_k)\n",
    "\n",
    "def calc_b_j_new(delta_j, b_j_last):\n",
    "    return b_j_last + learning_rate*np.sum(delta_j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "xhA9EPch_fNA"
   },
   "outputs": [],
   "source": [
    "# RMSE\n",
    "def rmse(predictions, targets):\n",
    "    return 100 - np.sqrt(((predictions - targets) ** 2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "P1VIQt4p_fNB"
   },
   "outputs": [],
   "source": [
    "# Train Model\n",
    "def train_model(train_count, w_train, v_train, b_j_train, b_k_train, y_train, x_train):\n",
    "    index = []\n",
    "    error = []\n",
    "    for l in range(train_count):\n",
    "        z_prediction = calc_z_j(x_train, w_train, b_j_train)\n",
    "        z_prediction = f_activation(z_prediction)\n",
    "        \n",
    "        y_prediction = calc_y_k(z_prediction, v_train, b_k_train)\n",
    "        y_prediction = f_activation_output(y_prediction)\n",
    "        \n",
    "        v_train, delta_k = calc_v_new(v_train, y_train, y_prediction, z_prediction)\n",
    "        w_train, delta_j = calc_w_new(w_train, v_train, delta_k, z_prediction, x_train)\n",
    "        \n",
    "        b_k_train = calc_b_k_new(delta_k, b_k_train)\n",
    "        b_j_train = calc_b_j_new(delta_j, b_j_train)\n",
    "        \n",
    "        rms = rmse(y_prediction, y_train)\n",
    "        \n",
    "        loss = error_calc(y_train, y_prediction)\n",
    "#         print(loss)\n",
    "\n",
    "        if l % 100 == 0:\n",
    "            error.append(loss)\n",
    "            index.append(l)\n",
    "            print(\"Iterasi ke-\" + str(l) + \" Error : \" + str(loss))\n",
    "            \n",
    "        if loss == 0.0001:\n",
    "            break\n",
    "        \n",
    "    plt.figure()\n",
    "    plt.plot(index, error)\n",
    "    plt.xlabel(\"Number of iterations\")\n",
    "    plt.ylabel(\"Cost\")\n",
    "    plt.show()\n",
    "    \n",
    "    print('-'*30)\n",
    "    print('TRAINING MODEL')\n",
    "    print('-'*30)\n",
    "    print('\\nOutput prediksi hidden layer : ')\n",
    "    print('-'*30)\n",
    "    print(z_prediction)\n",
    "    print('\\nOutput prediksi output layer : ')\n",
    "    print('-'*30)\n",
    "    print(y_prediction)\n",
    "    print('\\nOutput bobot dari hidden layer ke output layer : ')\n",
    "    print('-'*30)\n",
    "    print(v_train)\n",
    "    print('\\nOutput bobot dari input layer ke hidden layer : ')\n",
    "    print('-'*30)\n",
    "    print(w_train)\n",
    "    print('\\nOutput bias dari hidden layer ke output layer : ')\n",
    "    print('-'*30)\n",
    "    print(b_k_train)\n",
    "    print('\\nOutput bias dari input layer ke hidden layer : ')\n",
    "    print('-'*30)\n",
    "    print(b_j_train)\n",
    "    print('-'*30)\n",
    "    print()\n",
    "    print('-'*30)\n",
    "    print('AKURASI : ' + str(rms) + '%')\n",
    "    print('-'*30)\n",
    "    return v_train, w_train, b_k_train, b_j_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Ca-VgaUr_fNC",
    "outputId": "801aba44-55fb-46d7-b923-b7bddbd2a1e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterasi ke-0 Error : 0.03995795\n",
      "Iterasi ke-100 Error : 0.04046179\n",
      "Iterasi ke-200 Error : 0.040797208\n",
      "Iterasi ke-300 Error : 0.040586207\n",
      "Iterasi ke-400 Error : 0.04036117\n",
      "Iterasi ke-500 Error : 0.040118877\n",
      "Iterasi ke-600 Error : 0.03985458\n",
      "Iterasi ke-700 Error : 0.039564002\n",
      "Iterasi ke-800 Error : 0.039242987\n",
      "Iterasi ke-900 Error : 0.038887616\n",
      "Iterasi ke-1000 Error : 0.03849423\n",
      "Iterasi ke-1100 Error : 0.03805938\n",
      "Iterasi ke-1200 Error : 0.037580054\n",
      "Iterasi ke-1300 Error : 0.037053484\n",
      "Iterasi ke-1400 Error : 0.036477383\n",
      "Iterasi ke-1500 Error : 0.035849944\n",
      "Iterasi ke-1600 Error : 0.035169832\n",
      "Iterasi ke-1700 Error : 0.034436375\n",
      "Iterasi ke-1800 Error : 0.03364954\n",
      "Iterasi ke-1900 Error : 0.032809872\n",
      "Iterasi ke-2000 Error : 0.031918705\n",
      "Iterasi ke-2100 Error : 0.030978104\n",
      "Iterasi ke-2200 Error : 0.02999084\n",
      "Iterasi ke-2300 Error : 0.028960248\n",
      "Iterasi ke-2400 Error : 0.027890617\n",
      "Iterasi ke-2500 Error : 0.026786577\n",
      "Iterasi ke-2600 Error : 0.025653394\n",
      "Iterasi ke-2700 Error : 0.024496866\n",
      "Iterasi ke-2800 Error : 0.023323093\n",
      "Iterasi ke-2900 Error : 0.022138428\n",
      "Iterasi ke-3000 Error : 0.020949457\n",
      "Iterasi ke-3100 Error : 0.01976281\n",
      "Iterasi ke-3200 Error : 0.018585004\n",
      "Iterasi ke-3300 Error : 0.017422356\n",
      "Iterasi ke-3400 Error : 0.016280986\n",
      "Iterasi ke-3500 Error : 0.015166611\n",
      "Iterasi ke-3600 Error : 0.014084429\n",
      "Iterasi ke-3700 Error : 0.013039124\n",
      "Iterasi ke-3800 Error : 0.012034702\n",
      "Iterasi ke-3900 Error : 0.011074665\n",
      "Iterasi ke-4000 Error : 0.010161707\n",
      "Iterasi ke-4100 Error : 0.009297832\n",
      "Iterasi ke-4200 Error : 0.00848445\n",
      "Iterasi ke-4300 Error : 0.0077223005\n",
      "Iterasi ke-4400 Error : 0.0070114266\n",
      "Iterasi ke-4500 Error : 0.0063513317\n",
      "Iterasi ke-4600 Error : 0.0057410584\n",
      "Iterasi ke-4700 Error : 0.005179193\n",
      "Iterasi ke-4800 Error : 0.0046639056\n",
      "Iterasi ke-4900 Error : 0.004193105\n",
      "Iterasi ke-5000 Error : 0.003764467\n",
      "Iterasi ke-5100 Error : 0.003375512\n",
      "Iterasi ke-5200 Error : 0.0030236572\n",
      "Iterasi ke-5300 Error : 0.002706258\n",
      "Iterasi ke-5400 Error : 0.002420675\n",
      "Iterasi ke-5500 Error : 0.0021643403\n",
      "Iterasi ke-5600 Error : 0.0019347228\n",
      "Iterasi ke-5700 Error : 0.0017294708\n",
      "Iterasi ke-5800 Error : 0.0015462477\n",
      "Iterasi ke-5900 Error : 0.0013829378\n",
      "Iterasi ke-6000 Error : 0.0012375639\n",
      "Iterasi ke-6100 Error : 0.0011082552\n",
      "Iterasi ke-6200 Error : 0.000993346\n",
      "Iterasi ke-6300 Error : 0.0008912603\n",
      "Iterasi ke-6400 Error : 0.00080060534\n",
      "Iterasi ke-6500 Error : 0.00072011066\n",
      "Iterasi ke-6600 Error : 0.0006486283\n",
      "Iterasi ke-6700 Error : 0.00058513746\n",
      "Iterasi ke-6800 Error : 0.00052871986\n",
      "Iterasi ke-6900 Error : 0.00047855408\n",
      "Iterasi ke-7000 Error : 0.00043392248\n",
      "Iterasi ke-7100 Error : 0.0003941717\n",
      "Iterasi ke-7200 Error : 0.00035872095\n",
      "Iterasi ke-7300 Error : 0.00032708177\n",
      "Iterasi ke-7400 Error : 0.00029881464\n",
      "Iterasi ke-7500 Error : 0.00027351317\n",
      "Iterasi ke-7600 Error : 0.0002508291\n",
      "Iterasi ke-7700 Error : 0.00023046596\n",
      "Iterasi ke-7800 Error : 0.00021215157\n",
      "Iterasi ke-7900 Error : 0.00019565505\n",
      "Iterasi ke-8000 Error : 0.00018076441\n",
      "Iterasi ke-8100 Error : 0.00016730044\n",
      "Iterasi ke-8200 Error : 0.0001551009\n",
      "Iterasi ke-8300 Error : 0.00014402397\n",
      "Iterasi ke-8400 Error : 0.00013394404\n",
      "Iterasi ke-8500 Error : 0.00012475737\n",
      "Iterasi ke-8600 Error : 0.00011636303\n",
      "Iterasi ke-8700 Error : 0.00010867715\n",
      "Iterasi ke-8800 Error : 0.0001016257\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "TRAINING MODEL\n",
      "------------------------------\n",
      "\n",
      "Output prediksi hidden layer : \n",
      "------------------------------\n",
      "[[0.52687764 0.52687764 0.52687764 0.52687764 0.52687764]\n",
      " [0.50302815 0.50302815 0.50302815 0.50302815 0.50302815]\n",
      " [0.41564026 0.41564026 0.41564026 0.41564026 0.41564026]\n",
      " [0.6636436  0.6636436  0.6636436  0.6636436  0.6636436 ]\n",
      " [0.556524   0.556524   0.556524   0.556524   0.556524  ]\n",
      " [0.523057   0.523057   0.523057   0.523057   0.523057  ]\n",
      " [0.6259221  0.6259221  0.6259221  0.6259221  0.6259221 ]\n",
      " [0.6091771  0.6091771  0.6091771  0.6091771  0.6091771 ]\n",
      " [0.73179203 0.73179203 0.73179203 0.73179203 0.73179203]\n",
      " [0.4765388  0.4765388  0.4765388  0.4765388  0.4765388 ]\n",
      " [0.7013143  0.7013143  0.7013143  0.7013143  0.7013143 ]\n",
      " [0.618436   0.618436   0.618436   0.618436   0.618436  ]\n",
      " [0.57760966 0.57760966 0.57760966 0.57760966 0.57760966]\n",
      " [0.65223783 0.65223783 0.65223783 0.65223783 0.6522379 ]\n",
      " [0.6147465  0.6147465  0.6147465  0.6147465  0.6147465 ]\n",
      " [0.7105261  0.7105261  0.7105261  0.7105261  0.7105261 ]\n",
      " [0.6959371  0.6959371  0.6959371  0.6959371  0.6959371 ]\n",
      " [0.7442812  0.7442812  0.7442812  0.7442812  0.7442812 ]\n",
      " [0.51641214 0.51641214 0.51641214 0.51641214 0.51641214]\n",
      " [0.7272329  0.7272329  0.7272329  0.7272329  0.7272329 ]\n",
      " [0.804542   0.804542   0.804542   0.804542   0.804542  ]\n",
      " [0.7393371  0.7393371  0.7393371  0.7393371  0.7393371 ]\n",
      " [0.8024054  0.8024054  0.8024054  0.8024054  0.8024054 ]\n",
      " [0.7838159  0.7838159  0.7838159  0.7838159  0.7838159 ]]\n",
      "\n",
      "Output prediksi output layer : \n",
      "------------------------------\n",
      "[[0.29649282]\n",
      " [0.23800969]\n",
      " [0.02371895]\n",
      " [0.6318674 ]\n",
      " [0.36919093]\n",
      " [0.28712392]\n",
      " [0.53936756]\n",
      " [0.4983058 ]\n",
      " [0.7989795 ]\n",
      " [0.17305303]\n",
      " [0.72424257]\n",
      " [0.5210103 ]\n",
      " [0.42089677]\n",
      " [0.6038984 ]\n",
      " [0.511963  ]\n",
      " [0.74683166]\n",
      " [0.7110568 ]\n",
      " [0.8296051 ]\n",
      " [0.27082956]\n",
      " [0.7877997 ]\n",
      " [0.9773755 ]\n",
      " [0.8174814 ]\n",
      " [0.97213614]\n",
      " [0.92655146]]\n",
      "\n",
      "Output bobot dari hidden layer ke output layer : \n",
      "------------------------------\n",
      "[[0.49043956 0.49043956 0.49043956 0.49043956 0.49043956]]\n",
      "\n",
      "Output bobot dari input layer ke hidden layer : \n",
      "------------------------------\n",
      "[[0.28648233 0.06482154 0.23356047 0.22605951 0.28859115 0.30567077\n",
      "  0.14851105 0.17691098 0.23894025 0.10422292 0.17892598]\n",
      " [0.28648233 0.06482154 0.23356047 0.22605951 0.28859115 0.30567077\n",
      "  0.14851105 0.17691098 0.23894025 0.10422292 0.17892598]\n",
      " [0.28648233 0.06482154 0.23356047 0.22605951 0.28859115 0.30567077\n",
      "  0.14851105 0.17691098 0.23894025 0.10422292 0.17892598]\n",
      " [0.28648233 0.06482154 0.23356047 0.22605951 0.28859115 0.30567077\n",
      "  0.14851105 0.17691098 0.23894025 0.10422292 0.17892598]\n",
      " [0.28648233 0.06482154 0.23356047 0.22605951 0.28859115 0.30567077\n",
      "  0.14851102 0.17691098 0.23894027 0.10422293 0.17892598]]\n",
      "\n",
      "Output bias dari hidden layer ke output layer : \n",
      "------------------------------\n",
      "[-0.9955159]\n",
      "\n",
      "Output bias dari input layer ke hidden layer : \n",
      "------------------------------\n",
      "[-0.53128517 -0.53128517 -0.53128517 -0.53128517 -0.53128517]\n",
      "------------------------------\n",
      "\n",
      "------------------------------\n",
      "AKURASI : 99.62327039241791%\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Call function \n",
    "v, w, b_k, b_j = train_model(TRAIN_ITER, w, v, b_j, b_k, y, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "svN4Vgbb_fNE",
    "outputId": "713539c3-8819-4c6c-f3bf-c85f16eb4f73"
   },
   "outputs": [],
   "source": [
    "# Test Model\n",
    "def test(x_test, y_test, w, v,  b_j, b_k):\n",
    "    z_prediction = np.dot(x_test, np.transpose(w)) + b_j\n",
    "    f_z_prediction = f_activation(z_prediction)\n",
    "    \n",
    "    y_prediction = np.dot(f_z_prediction, np.transpose(v)) + b_k\n",
    "    f_y_prediction =  f_activation_output(y_prediction)\n",
    "    \n",
    "    rms = rmse(f_y_prediction, y_test)\n",
    "    \n",
    "    print('-'*30)\n",
    "    print('TESTING MODEL')\n",
    "    print('-'*30)\n",
    "    print('\\nOutput prediksi hidden layer : ')\n",
    "    print('-'*30)\n",
    "    print(z_prediction)\n",
    "    print('\\nOutput prediksi output layer : ')\n",
    "    print('-'*30)\n",
    "    print(y_prediction)\n",
    "    print('\\nOutput bobot dari hidden layer ke output layer : ')\n",
    "    print('-'*30)\n",
    "    print(v)\n",
    "    print('\\nOutput bobot dari input layer ke hidden layer : ')\n",
    "    print('-'*30)\n",
    "    print(w)\n",
    "    print('\\nOutput bias dari hidden layer ke output layer : ')\n",
    "    print('-'*30)\n",
    "    print(b_k)\n",
    "    print('\\nOutput bias dari input layer ke hidden layer : ')\n",
    "    print('-'*30)\n",
    "    print(b_j)\n",
    "    print('-'*30)\n",
    "    print()\n",
    "    print('-'*30)\n",
    "    print('AKURASI : ' + str(rms) + '%')\n",
    "    print('-'*30)\n",
    "    return f_y_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "TESTING MODEL\n",
      "------------------------------\n",
      "\n",
      "Output prediksi hidden layer : \n",
      "------------------------------\n",
      "[[ 0.3499874   0.3499874   0.3499874   0.3499874   0.3499874 ]\n",
      " [ 0.37802374  0.37802374  0.37802374  0.37802374  0.37802374]\n",
      " [-0.1538724  -0.1538724  -0.1538724  -0.1538724  -0.1538724 ]\n",
      " [ 0.35689682  0.35689682  0.35689682  0.35689682  0.35689688]\n",
      " [ 0.47579992  0.47579992  0.47579992  0.47579992  0.4757998 ]\n",
      " [ 1.1113073   1.1113073   1.1113073   1.1113073   1.1113073 ]\n",
      " [-0.50275695 -0.50275695 -0.50275695 -0.50275695 -0.50275695]\n",
      " [ 0.9335753   0.9335753   0.9335753   0.9335753   0.9335753 ]\n",
      " [ 0.38080883  0.38080883  0.38080883  0.38080883  0.38080877]\n",
      " [ 0.4827187   0.4827187   0.4827187   0.4827187   0.4827187 ]\n",
      " [ 1.3909485   1.3909485   1.3909485   1.3909485   1.3909485 ]\n",
      " [ 0.5891882   0.5891882   0.5891882   0.5891882   0.5891881 ]]\n",
      "\n",
      "Output prediksi output layer : \n",
      "------------------------------\n",
      "[[ 0.44297892]\n",
      " [ 0.45960933]\n",
      " [ 0.13643736]\n",
      " [ 0.44708532]\n",
      " [ 0.5168908 ]\n",
      " [ 0.849451  ]\n",
      " [-0.07129973]\n",
      " [ 0.76467186]\n",
      " [ 0.4612568 ]\n",
      " [ 0.5208979 ]\n",
      " [ 0.9680659 ]\n",
      " [ 0.58168584]]\n",
      "\n",
      "Output bobot dari hidden layer ke output layer : \n",
      "------------------------------\n",
      "[[0.49043956 0.49043956 0.49043956 0.49043956 0.49043956]]\n",
      "\n",
      "Output bobot dari input layer ke hidden layer : \n",
      "------------------------------\n",
      "[[0.28648233 0.06482154 0.23356047 0.22605951 0.28859115 0.30567077\n",
      "  0.14851105 0.17691098 0.23894025 0.10422292 0.17892598]\n",
      " [0.28648233 0.06482154 0.23356047 0.22605951 0.28859115 0.30567077\n",
      "  0.14851105 0.17691098 0.23894025 0.10422292 0.17892598]\n",
      " [0.28648233 0.06482154 0.23356047 0.22605951 0.28859115 0.30567077\n",
      "  0.14851105 0.17691098 0.23894025 0.10422292 0.17892598]\n",
      " [0.28648233 0.06482154 0.23356047 0.22605951 0.28859115 0.30567077\n",
      "  0.14851105 0.17691098 0.23894025 0.10422292 0.17892598]\n",
      " [0.28648233 0.06482154 0.23356047 0.22605951 0.28859115 0.30567077\n",
      "  0.14851102 0.17691098 0.23894027 0.10422293 0.17892598]]\n",
      "\n",
      "Output bias dari hidden layer ke output layer : \n",
      "------------------------------\n",
      "[-0.9955159]\n",
      "\n",
      "Output bias dari input layer ke hidden layer : \n",
      "------------------------------\n",
      "[-0.53128517 -0.53128517 -0.53128517 -0.53128517 -0.53128517]\n",
      "------------------------------\n",
      "\n",
      "------------------------------\n",
      "AKURASI : 99.61881390213966%\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Call function\n",
    "y_prediction_test = np.zeros(len(xx), np.float32)\n",
    "y_prediction_test = test(xx, yy, w, v,  b_j, b_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PRINT' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-164e45ff57bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mPRINT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'PRINT' is not defined"
     ]
    }
   ],
   "source": [
    "PRINT(ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DaoqXNOt_fNF",
    "outputId": "76d6d067-1753-47b0-fd93-4dca66d50e8f"
   },
   "outputs": [],
   "source": [
    "# Denormalize prediction\n",
    "j = datatest['J']\n",
    "denorm = []\n",
    "for i in range(len(data_test)):\n",
    "    y[i] = y_prediction_test[i] * (j.max(axis=0) - j.min(axis=0)) + j.min(axis=0) \n",
    "    denorm.append(y[i])\n",
    "print(denorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "CNSU4JWN_fNG",
    "outputId": "edb6b7be-fa49-4b8a-f9fc-bc0c7df6e8b4"
   },
   "outputs": [],
   "source": [
    "# Output prediction File\n",
    "submission = pd.DataFrame({\n",
    "    \"Time\" : datatest['B'],\n",
    "    \"Result\": denorm\n",
    "    })\n",
    "\n",
    "submission.head(5)\n",
    "\n",
    "# Export it in a 'Comma Separated Values' (CSV) file\n",
    "submission.to_csv(r'/home/systemcommand/anggy/app/data/pred(11-5-1-8900).csv', index=False)\n",
    "# Creating a link to download the .csv file we created\n",
    "from IPython.display import FileLink\n",
    "FileLink(r'/home/systemcommand/anggy/app/data/pred(11-5-1-8900).csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "BZrSEM1m_fNH",
    "outputId": "62d0aae7-a511-4738-e087-490c7f5f7763"
   },
   "outputs": [],
   "source": [
    "# Output Truth File\n",
    "submission = pd.DataFrame({\n",
    "    \"Time\" : datatest['B'],\n",
    "    \"Result\": datatest['J']\n",
    "    })\n",
    "\n",
    "submission.head(5)\n",
    "\n",
    "# Export it in a 'Comma Separated Values' (CSV) file\n",
    "submission.to_csv(r'/home/systemcommand/anggy/app/data/truth.csv', index=False)\n",
    "# Creating a link to download the .csv file we created\n",
    "from IPython.display import FileLink\n",
    "FileLink(r'/home/systemcommand/anggy/app/data/truth.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 523
    },
    "id": "dzYCYW0z_fNI",
    "outputId": "82a40adf-c9b7-443d-be86-a36ece51664c"
   },
   "outputs": [],
   "source": [
    "# Plot prediction and truth data\n",
    "dataa = pd.read_csv('/home/systemcommand/anggy/app/data/truth.csv')\n",
    "datac = pd.read_csv('/home/systemcommand/anggy/app/data/pred(11-5-1-8900).csv')\n",
    "\n",
    "x = dataa['Result']\n",
    "y = dataa['Time']\n",
    "\n",
    "x2 = datac['Result']\n",
    "y2 = datac['Time']\n",
    "\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.scatter(y2,x2,c='red')\n",
    "plt.plot(y2,x2,c='red', label=\"Prediction\")\n",
    "plt.scatter(y,x,c='blue')\n",
    "plt.plot(y,x,c='blue', label=\"Actual\")\n",
    "plt.legend(loc='upper left', fontsize = 20)\n",
    "plt.ylabel('prediction', fontsize = 15)\n",
    "plt.xlabel('period', fontsize = 15)\n",
    "plt.title('Perbandingan data aktual dan prediksi 2019', fontsize = 20)\n",
    "plt.xticks()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "5-15000.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
