{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "nuhQNHqS_fMn"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "id": "PWOkvP6F_fM0",
    "outputId": "216de9e8-74fa-479b-81f6-5d07370d1805"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>T</th>\n",
       "      <th>B</th>\n",
       "      <th>SU1</th>\n",
       "      <th>SU2</th>\n",
       "      <th>RT1</th>\n",
       "      <th>RT2</th>\n",
       "      <th>RT3</th>\n",
       "      <th>RT4</th>\n",
       "      <th>I1</th>\n",
       "      <th>I2</th>\n",
       "      <th>N1</th>\n",
       "      <th>N2</th>\n",
       "      <th>N3</th>\n",
       "      <th>J</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017</td>\n",
       "      <td>Januari</td>\n",
       "      <td>14835.0</td>\n",
       "      <td>18172.0</td>\n",
       "      <td>14183.0</td>\n",
       "      <td>24728.0</td>\n",
       "      <td>841756.0</td>\n",
       "      <td>201048.0</td>\n",
       "      <td>46800.0</td>\n",
       "      <td>39938.0</td>\n",
       "      <td>56324.0</td>\n",
       "      <td>32492.0</td>\n",
       "      <td>5994.0</td>\n",
       "      <td>1296270.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017</td>\n",
       "      <td>Februari</td>\n",
       "      <td>14893.0</td>\n",
       "      <td>19026.0</td>\n",
       "      <td>14371.0</td>\n",
       "      <td>24394.0</td>\n",
       "      <td>832284.0</td>\n",
       "      <td>200774.0</td>\n",
       "      <td>47536.0</td>\n",
       "      <td>37282.0</td>\n",
       "      <td>57218.0</td>\n",
       "      <td>31412.0</td>\n",
       "      <td>5329.0</td>\n",
       "      <td>1284519.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017</td>\n",
       "      <td>Maret</td>\n",
       "      <td>13676.0</td>\n",
       "      <td>17706.0</td>\n",
       "      <td>13864.0</td>\n",
       "      <td>23071.0</td>\n",
       "      <td>789354.0</td>\n",
       "      <td>197318.0</td>\n",
       "      <td>41005.0</td>\n",
       "      <td>35755.0</td>\n",
       "      <td>55068.0</td>\n",
       "      <td>30468.0</td>\n",
       "      <td>5737.0</td>\n",
       "      <td>1223022.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017</td>\n",
       "      <td>April</td>\n",
       "      <td>15780.0</td>\n",
       "      <td>20222.0</td>\n",
       "      <td>15791.0</td>\n",
       "      <td>26153.0</td>\n",
       "      <td>893393.0</td>\n",
       "      <td>224847.0</td>\n",
       "      <td>47793.0</td>\n",
       "      <td>38759.0</td>\n",
       "      <td>63590.0</td>\n",
       "      <td>34198.0</td>\n",
       "      <td>6926.0</td>\n",
       "      <td>1387452.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017</td>\n",
       "      <td>Mei</td>\n",
       "      <td>15698.0</td>\n",
       "      <td>17508.0</td>\n",
       "      <td>14863.0</td>\n",
       "      <td>24681.0</td>\n",
       "      <td>851205.0</td>\n",
       "      <td>222635.0</td>\n",
       "      <td>45794.0</td>\n",
       "      <td>37655.0</td>\n",
       "      <td>60998.0</td>\n",
       "      <td>26452.0</td>\n",
       "      <td>5915.0</td>\n",
       "      <td>1323404.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      T         B      SU1      SU2      RT1      RT2       RT3       RT4  \\\n",
       "0  2017   Januari  14835.0  18172.0  14183.0  24728.0  841756.0  201048.0   \n",
       "1  2017  Februari  14893.0  19026.0  14371.0  24394.0  832284.0  200774.0   \n",
       "2  2017     Maret  13676.0  17706.0  13864.0  23071.0  789354.0  197318.0   \n",
       "3  2017     April  15780.0  20222.0  15791.0  26153.0  893393.0  224847.0   \n",
       "4  2017       Mei  15698.0  17508.0  14863.0  24681.0  851205.0  222635.0   \n",
       "\n",
       "        I1       I2       N1       N2      N3          J  \n",
       "0  46800.0  39938.0  56324.0  32492.0  5994.0  1296270.0  \n",
       "1  47536.0  37282.0  57218.0  31412.0  5329.0  1284519.0  \n",
       "2  41005.0  35755.0  55068.0  30468.0  5737.0  1223022.0  \n",
       "3  47793.0  38759.0  63590.0  34198.0  6926.0  1387452.0  \n",
       "4  45794.0  37655.0  60998.0  26452.0  5915.0  1323404.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data Train Load dataset\n",
    "data = pd.read_csv(\"/home/systemcommand/anggy/app/data/dataanggy.csv\", delimiter=',')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_g9T_SEU_fM2",
    "outputId": "75d9f810-842f-4476-f80d-e392b6b1d165"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 36 entries, 0 to 35\n",
      "Data columns (total 14 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   T       36 non-null     int64  \n",
      " 1   B       36 non-null     object \n",
      " 2   SU1     36 non-null     float64\n",
      " 3   SU2     36 non-null     float64\n",
      " 4   RT1     36 non-null     float64\n",
      " 5   RT2     36 non-null     float64\n",
      " 6   RT3     36 non-null     float64\n",
      " 7   RT4     36 non-null     float64\n",
      " 8   I1      36 non-null     float64\n",
      " 9   I2      36 non-null     float64\n",
      " 10  N1      36 non-null     float64\n",
      " 11  N2      36 non-null     float64\n",
      " 12  N3      36 non-null     float64\n",
      " 13  J       36 non-null     float64\n",
      "dtypes: float64(12), int64(1), object(1)\n",
      "memory usage: 4.1+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 304
    },
    "id": "lXyRijjI_fM3",
    "outputId": "be415e09-64e4-40d1-b573-63584d022939"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>T</th>\n",
       "      <th>SU1</th>\n",
       "      <th>SU2</th>\n",
       "      <th>RT1</th>\n",
       "      <th>RT2</th>\n",
       "      <th>RT3</th>\n",
       "      <th>RT4</th>\n",
       "      <th>I1</th>\n",
       "      <th>I2</th>\n",
       "      <th>N1</th>\n",
       "      <th>N2</th>\n",
       "      <th>N3</th>\n",
       "      <th>J</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>36.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>3.600000e+01</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>3.600000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2018.000000</td>\n",
       "      <td>16811.762626</td>\n",
       "      <td>18289.593434</td>\n",
       "      <td>17319.497475</td>\n",
       "      <td>25285.578283</td>\n",
       "      <td>8.746521e+05</td>\n",
       "      <td>268326.891414</td>\n",
       "      <td>51928.075758</td>\n",
       "      <td>43835.411616</td>\n",
       "      <td>65745.484848</td>\n",
       "      <td>33214.449495</td>\n",
       "      <td>6299.679293</td>\n",
       "      <td>1.421709e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.828079</td>\n",
       "      <td>1716.505834</td>\n",
       "      <td>1344.417407</td>\n",
       "      <td>2018.962672</td>\n",
       "      <td>1090.830689</td>\n",
       "      <td>5.313820e+04</td>\n",
       "      <td>36903.007764</td>\n",
       "      <td>4780.094874</td>\n",
       "      <td>5577.029476</td>\n",
       "      <td>6169.312152</td>\n",
       "      <td>3650.208830</td>\n",
       "      <td>535.692608</td>\n",
       "      <td>1.063310e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2017.000000</td>\n",
       "      <td>13676.000000</td>\n",
       "      <td>15291.000000</td>\n",
       "      <td>13584.000000</td>\n",
       "      <td>22892.000000</td>\n",
       "      <td>7.741400e+05</td>\n",
       "      <td>197318.000000</td>\n",
       "      <td>41005.000000</td>\n",
       "      <td>33836.000000</td>\n",
       "      <td>54898.000000</td>\n",
       "      <td>23525.000000</td>\n",
       "      <td>5329.000000</td>\n",
       "      <td>1.223022e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2017.000000</td>\n",
       "      <td>15665.500000</td>\n",
       "      <td>17628.000000</td>\n",
       "      <td>15844.250000</td>\n",
       "      <td>24706.500000</td>\n",
       "      <td>8.443638e+05</td>\n",
       "      <td>239055.750000</td>\n",
       "      <td>48611.000000</td>\n",
       "      <td>39885.000000</td>\n",
       "      <td>61232.000000</td>\n",
       "      <td>30393.000000</td>\n",
       "      <td>5906.000000</td>\n",
       "      <td>1.343209e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2018.000000</td>\n",
       "      <td>16859.000000</td>\n",
       "      <td>18305.500000</td>\n",
       "      <td>17569.500000</td>\n",
       "      <td>25380.000000</td>\n",
       "      <td>8.676585e+05</td>\n",
       "      <td>270760.000000</td>\n",
       "      <td>52107.500000</td>\n",
       "      <td>43078.500000</td>\n",
       "      <td>65386.000000</td>\n",
       "      <td>33711.000000</td>\n",
       "      <td>6153.000000</td>\n",
       "      <td>1.413994e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2019.000000</td>\n",
       "      <td>17664.750000</td>\n",
       "      <td>19032.500000</td>\n",
       "      <td>18850.227273</td>\n",
       "      <td>25993.250000</td>\n",
       "      <td>9.015332e+05</td>\n",
       "      <td>296272.500000</td>\n",
       "      <td>54216.250000</td>\n",
       "      <td>47648.250000</td>\n",
       "      <td>71050.863636</td>\n",
       "      <td>36266.545455</td>\n",
       "      <td>6847.750000</td>\n",
       "      <td>1.486746e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2019.000000</td>\n",
       "      <td>21841.000000</td>\n",
       "      <td>21044.000000</td>\n",
       "      <td>21566.000000</td>\n",
       "      <td>27466.000000</td>\n",
       "      <td>1.022240e+06</td>\n",
       "      <td>336887.000000</td>\n",
       "      <td>64193.000000</td>\n",
       "      <td>56427.000000</td>\n",
       "      <td>79014.000000</td>\n",
       "      <td>38926.000000</td>\n",
       "      <td>7422.000000</td>\n",
       "      <td>1.677296e+06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 T           SU1           SU2           RT1           RT2  \\\n",
       "count    36.000000     36.000000     36.000000     36.000000     36.000000   \n",
       "mean   2018.000000  16811.762626  18289.593434  17319.497475  25285.578283   \n",
       "std       0.828079   1716.505834   1344.417407   2018.962672   1090.830689   \n",
       "min    2017.000000  13676.000000  15291.000000  13584.000000  22892.000000   \n",
       "25%    2017.000000  15665.500000  17628.000000  15844.250000  24706.500000   \n",
       "50%    2018.000000  16859.000000  18305.500000  17569.500000  25380.000000   \n",
       "75%    2019.000000  17664.750000  19032.500000  18850.227273  25993.250000   \n",
       "max    2019.000000  21841.000000  21044.000000  21566.000000  27466.000000   \n",
       "\n",
       "                RT3            RT4            I1            I2            N1  \\\n",
       "count  3.600000e+01      36.000000     36.000000     36.000000     36.000000   \n",
       "mean   8.746521e+05  268326.891414  51928.075758  43835.411616  65745.484848   \n",
       "std    5.313820e+04   36903.007764   4780.094874   5577.029476   6169.312152   \n",
       "min    7.741400e+05  197318.000000  41005.000000  33836.000000  54898.000000   \n",
       "25%    8.443638e+05  239055.750000  48611.000000  39885.000000  61232.000000   \n",
       "50%    8.676585e+05  270760.000000  52107.500000  43078.500000  65386.000000   \n",
       "75%    9.015332e+05  296272.500000  54216.250000  47648.250000  71050.863636   \n",
       "max    1.022240e+06  336887.000000  64193.000000  56427.000000  79014.000000   \n",
       "\n",
       "                 N2           N3             J  \n",
       "count     36.000000    36.000000  3.600000e+01  \n",
       "mean   33214.449495  6299.679293  1.421709e+06  \n",
       "std     3650.208830   535.692608  1.063310e+05  \n",
       "min    23525.000000  5329.000000  1.223022e+06  \n",
       "25%    30393.000000  5906.000000  1.343209e+06  \n",
       "50%    33711.000000  6153.000000  1.413994e+06  \n",
       "75%    36266.545455  6847.750000  1.486746e+06  \n",
       "max    38926.000000  7422.000000  1.677296e+06  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# EDA/ data description\n",
    "this = data.copy()\n",
    "this.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "BW3EMuNm_fM3"
   },
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "train = this.drop(['T','B'],axis=1)\n",
    "\n",
    "# Preprocessing/ transform(normalizing)\n",
    "scaler = preprocessing.MinMaxScaler().fit(train)\n",
    "data_train = scaler.transform(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hcKi8waW_fM4",
    "outputId": "3b78b2df-e38b-439d-a000-afaad29ddb11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36, 11)\n"
     ]
    }
   ],
   "source": [
    "# Selecting Variables\n",
    "datax = []\n",
    "for i in range(len(data_train)):\n",
    "    tes = data_train[i][:11]\n",
    "    datax.append(tes)\n",
    "\n",
    "datay = []\n",
    "for i in range(len(data_train)):\n",
    "    tes = data_train[i][-1]\n",
    "    datay.append(tes)\n",
    "\n",
    "x = np.array(datax,np.float32)\n",
    "y = np.array(datay,np.float32)\n",
    "\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Nlz-v_OZ_fM8"
   },
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "n = x.shape[1]\n",
    "m = 5\n",
    "M = 1\n",
    "init_arr_item = 0.0\n",
    "\n",
    "learning_rate = 0.001\n",
    "TRAIN_ITER = 8900"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Rkx_Vmyc_fM9"
   },
   "outputs": [],
   "source": [
    "# Make Placeholder \n",
    "w = np.ones([m,n], np.float32)*init_arr_item\n",
    "v = np.ones([M,m], np.float32)*init_arr_item\n",
    "b_j = np.zeros([m], np.float32)*init_arr_item\n",
    "b_k = np.zeros([M], np.float32)*init_arr_item\n",
    "\n",
    "z_pred = np.zeros([y.shape[0], m], np.float32)\n",
    "y_pred = np.zeros([y.shape[0], M], np.float32)\n",
    "E = np.zeros(M, np.float32)\n",
    "\n",
    "v_new = np.zeros([M,m], np.float32)\n",
    "delta_k = np.zeros(M, np.float32)\n",
    "\n",
    "w_new = np.zeros([m,n], np.float32)\n",
    "delta_j = np.zeros(m, np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "BxMdoD83_fM9"
   },
   "outputs": [],
   "source": [
    "# Define Sigmoid activation function\n",
    "def f_sigmoid(value):\n",
    "    return 1.0/(1.0+np.exp(-value))\n",
    "\n",
    "def f_sigmoid_derivation(value):\n",
    "    return f_sigmoid(value)*(1-f_sigmoid(value))\n",
    "  \n",
    "# Define Linear activation function\n",
    "def f_linear(value):\n",
    "    return value\n",
    "\n",
    "def f_linear_derivation(value):\n",
    "    return np.ones(value.shape, np.float32)\n",
    "  \n",
    "# Hidden Layer activation function wrapper\n",
    "def f_activation(value):\n",
    "    return f_sigmoid(value)\n",
    "  \n",
    "def f_activation_derivation(value):\n",
    "    return f_sigmoid_derivation(value)\n",
    "\n",
    "# Output Layer activation function wrapper\n",
    "def f_activation_output(value):\n",
    "    return f_linear(value)\n",
    "\n",
    "def f_activation_derivation_output(value):\n",
    "    return f_linear_derivation(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "oWieVuy1_fM-"
   },
   "outputs": [],
   "source": [
    "# Feed Forward\n",
    "def calc_z_j(x_train, w_train, b_j_train):\n",
    "    for j in range(m):\n",
    "        z_pred[:,j] = np.dot(x_train,w_train[j,:]) + b_j_train[j]\n",
    "    \n",
    "    return z_pred\n",
    "    \n",
    "def calc_y_k(z_train, v_train, b_k_train):\n",
    "    for k in range(M):\n",
    "        y_pred[:,k] = np.dot(z_train, v_train[k,:]) + b_k_train[k]\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "FLSx3bXg_fM_"
   },
   "outputs": [],
   "source": [
    "# Calculate Loss\n",
    "def error_calc(target, output):\n",
    "    for k in range(M):\n",
    "        E[k] = 0.5*(target[k] - output[k])**2\n",
    "    return np.sum(E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "ALOQ5-Ct_fM_"
   },
   "outputs": [],
   "source": [
    "# Backpropagation\n",
    "def calc_v_new(v_last, y, y_prediction, z_prediction):\n",
    "    delta_k = np.transpose(f_activation_derivation_output(y_prediction)*(np.reshape(y, (y.shape[0], 1)) - y_prediction))\n",
    "        \n",
    "    for j in range(m):\n",
    "        v_new[:,j] = v_last[:,j] + learning_rate*np.dot(delta_k,z_prediction[:,j])\n",
    "            \n",
    "    return v_new, delta_k\n",
    "\n",
    "\n",
    "def calc_w_new(w_last, v_new, delta_k, z_prediction, x_input):\n",
    "    sum_v = np.dot(np.transpose(v_new), delta_k)\n",
    "    \n",
    "    delta_j = np.transpose(f_activation_derivation(z_prediction))*sum_v\n",
    "    \n",
    "    for i in range(n):\n",
    "            w_new[:,i] = w_last[:,i] + learning_rate*np.dot(delta_j,x_input[:,i])\n",
    "            \n",
    "    return w_new, delta_j\n",
    "    \n",
    "def calc_b_k_new(delta_k, b_k_last):\n",
    "    return b_k_last +  learning_rate*np.sum(delta_k)\n",
    "\n",
    "def calc_b_j_new(delta_j, b_j_last):\n",
    "    return b_j_last + learning_rate*np.sum(delta_j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "xhA9EPch_fNA"
   },
   "outputs": [],
   "source": [
    "# RMSE\n",
    "def rmse(predictions, targets):\n",
    "    return 100 - np.sqrt(((predictions - targets) ** 2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "P1VIQt4p_fNB"
   },
   "outputs": [],
   "source": [
    "# Train Model\n",
    "def train_model(train_count, w_train, v_train, b_j_train, b_k_train, y_train, x_train):\n",
    "    index = []\n",
    "    error = []\n",
    "    for l in range(train_count):\n",
    "        z_prediction = calc_z_j(x_train, w_train, b_j_train)\n",
    "        z_prediction = f_activation(z_prediction)\n",
    "        \n",
    "        y_prediction = calc_y_k(z_prediction, v_train, b_k_train)\n",
    "        y_prediction = f_activation_output(y_prediction)\n",
    "        \n",
    "        v_train, delta_k = calc_v_new(v_train, y_train, y_prediction, z_prediction)\n",
    "        w_train, delta_j = calc_w_new(w_train, v_train, delta_k, z_prediction, x_train)\n",
    "        \n",
    "        b_k_train = calc_b_k_new(delta_k, b_k_train)\n",
    "        b_j_train = calc_b_j_new(delta_j, b_j_train)\n",
    "        \n",
    "        rms = rmse(y_prediction, y_train)\n",
    "        \n",
    "        loss = error_calc(y_train, y_prediction)\n",
    "\n",
    "        if l % 100 == 0:\n",
    "            error.append(loss)\n",
    "            index.append(l)\n",
    "            print(\"Iterasi ke-\" + str(l) + \" Error : \" + str(loss))\n",
    "            \n",
    "        if loss == 0.0001:\n",
    "            break\n",
    "        \n",
    "    plt.figure()\n",
    "    plt.plot(index, error)\n",
    "    plt.xlabel(\"Number of iterations\")\n",
    "    plt.ylabel(\"Cost\")\n",
    "    plt.show()\n",
    "    \n",
    "    print('-'*30)\n",
    "    print('TRAINING MODEL')\n",
    "    print('-'*30)\n",
    "    print('\\nOutput prediksi hidden layer : ')\n",
    "    print('-'*30)\n",
    "    print(z_prediction)\n",
    "    print('\\nOutput prediksi output layer : ')\n",
    "    print('-'*30)\n",
    "    print(y_prediction)\n",
    "    print('\\nOutput bobot dari hidden layer ke output layer : ')\n",
    "    print('-'*30)\n",
    "    print(v_train)\n",
    "    print('\\nOutput bobot dari input layer ke hidden layer : ')\n",
    "    print('-'*30)\n",
    "    print(w_train)\n",
    "    print('\\nOutput bias dari hidden layer ke output layer : ')\n",
    "    print('-'*30)\n",
    "    print(b_k_train)\n",
    "    print('\\nOutput bias dari input layer ke hidden layer : ')\n",
    "    print('-'*30)\n",
    "    print(b_j_train)\n",
    "    print('-'*30)\n",
    "    print()\n",
    "    print('-'*30)\n",
    "    print('AKURASI : ' + str(rms) + '%')\n",
    "    print('-'*30)\n",
    "    return v_train, w_train, b_k_train, b_j_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Ca-VgaUr_fNC",
    "outputId": "801aba44-55fb-46d7-b923-b7bddbd2a1e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterasi ke-0 Error : 0.012999473\n",
      "Iterasi ke-100 Error : 0.037952293\n",
      "Iterasi ke-200 Error : 0.037790656\n",
      "Iterasi ke-300 Error : 0.037596643\n",
      "Iterasi ke-400 Error : 0.03738573\n",
      "Iterasi ke-500 Error : 0.037151594\n",
      "Iterasi ke-600 Error : 0.036888067\n",
      "Iterasi ke-700 Error : 0.03658901\n",
      "Iterasi ke-800 Error : 0.03624836\n",
      "Iterasi ke-900 Error : 0.035860106\n",
      "Iterasi ke-1000 Error : 0.03541834\n",
      "Iterasi ke-1100 Error : 0.034917414\n",
      "Iterasi ke-1200 Error : 0.034352027\n",
      "Iterasi ke-1300 Error : 0.033717338\n",
      "Iterasi ke-1400 Error : 0.033009212\n",
      "Iterasi ke-1500 Error : 0.032224424\n",
      "Iterasi ke-1600 Error : 0.03136073\n",
      "Iterasi ke-1700 Error : 0.030417247\n",
      "Iterasi ke-1800 Error : 0.029394513\n",
      "Iterasi ke-1900 Error : 0.028294636\n",
      "Iterasi ke-2000 Error : 0.027121447\n",
      "Iterasi ke-2100 Error : 0.025880497\n",
      "Iterasi ke-2200 Error : 0.024579043\n",
      "Iterasi ke-2300 Error : 0.02322602\n",
      "Iterasi ke-2400 Error : 0.021831792\n",
      "Iterasi ke-2500 Error : 0.020407958\n",
      "Iterasi ke-2600 Error : 0.018967068\n",
      "Iterasi ke-2700 Error : 0.017522363\n",
      "Iterasi ke-2800 Error : 0.016087186\n",
      "Iterasi ke-2900 Error : 0.014674836\n",
      "Iterasi ke-3000 Error : 0.013297977\n",
      "Iterasi ke-3100 Error : 0.0119684115\n",
      "Iterasi ke-3200 Error : 0.010696686\n",
      "Iterasi ke-3300 Error : 0.009491871\n",
      "Iterasi ke-3400 Error : 0.008361318\n",
      "Iterasi ke-3500 Error : 0.007310463\n",
      "Iterasi ke-3600 Error : 0.0063430145\n",
      "Iterasi ke-3700 Error : 0.0054607224\n",
      "Iterasi ke-3800 Error : 0.004663615\n",
      "Iterasi ke-3900 Error : 0.0039502196\n",
      "Iterasi ke-4000 Error : 0.0033176327\n",
      "Iterasi ke-4100 Error : 0.0027618743\n",
      "Iterasi ke-4200 Error : 0.0022781047\n",
      "Iterasi ke-4300 Error : 0.001860852\n",
      "Iterasi ke-4400 Error : 0.0015042962\n",
      "Iterasi ke-4500 Error : 0.0012024705\n",
      "Iterasi ke-4600 Error : 0.0009494262\n",
      "Iterasi ke-4700 Error : 0.000739387\n",
      "Iterasi ke-4800 Error : 0.0005668869\n",
      "Iterasi ke-4900 Error : 0.00042681495\n",
      "Iterasi ke-5000 Error : 0.00031449486\n",
      "Iterasi ke-5100 Error : 0.00022572067\n",
      "Iterasi ke-5200 Error : 0.0001567297\n",
      "Iterasi ke-5300 Error : 0.000104218074\n",
      "Iterasi ke-5400 Error : 6.5309796e-05\n",
      "Iterasi ke-5500 Error : 3.7526355e-05\n",
      "Iterasi ke-5600 Error : 1.8753633e-05\n",
      "Iterasi ke-5700 Error : 7.2060952e-06\n",
      "Iterasi ke-5800 Error : 1.3843203e-06\n",
      "Iterasi ke-5900 Error : 4.145761e-08\n",
      "Iterasi ke-6000 Error : 2.150003e-06\n",
      "Iterasi ke-6100 Error : 6.868684e-06\n",
      "Iterasi ke-6200 Error : 1.351364e-05\n",
      "Iterasi ke-6300 Error : 2.1536927e-05\n",
      "Iterasi ke-6400 Error : 3.049942e-05\n",
      "Iterasi ke-6500 Error : 4.005654e-05\n",
      "Iterasi ke-6600 Error : 4.993734e-05\n",
      "Iterasi ke-6700 Error : 5.993264e-05\n",
      "Iterasi ke-6800 Error : 6.988813e-05\n",
      "Iterasi ke-6900 Error : 7.9683006e-05\n",
      "Iterasi ke-7000 Error : 8.923556e-05\n",
      "Iterasi ke-7100 Error : 9.848373e-05\n",
      "Iterasi ke-7200 Error : 0.00010739228\n",
      "Iterasi ke-7300 Error : 0.00011593379\n",
      "Iterasi ke-7400 Error : 0.00012410058\n",
      "Iterasi ke-7500 Error : 0.00013188817\n",
      "Iterasi ke-7600 Error : 0.00013930417\n",
      "Iterasi ke-7700 Error : 0.00014635548\n",
      "Iterasi ke-7800 Error : 0.00015306317\n",
      "Iterasi ke-7900 Error : 0.00015943543\n",
      "Iterasi ke-8000 Error : 0.00016549058\n",
      "Iterasi ke-8100 Error : 0.00017124797\n",
      "Iterasi ke-8200 Error : 0.00017672262\n",
      "Iterasi ke-8300 Error : 0.00018193987\n",
      "Iterasi ke-8400 Error : 0.0001869125\n",
      "Iterasi ke-8500 Error : 0.00019166265\n",
      "Iterasi ke-8600 Error : 0.00019619131\n",
      "Iterasi ke-8700 Error : 0.00020052926\n",
      "Iterasi ke-8800 Error : 0.00020468303\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "TRAINING MODEL\n",
      "------------------------------\n",
      "\n",
      "Output prediksi hidden layer : \n",
      "------------------------------\n",
      "[[0.49229896 0.49229896 0.49229896 0.49229896 0.49229902]\n",
      " [0.4880514  0.4880514  0.4880514  0.4880514  0.4880514 ]\n",
      " [0.41841748 0.41841748 0.41841748 0.41841748 0.41841757]\n",
      " [0.5861329  0.5861329  0.5861329  0.5861329  0.58613294]\n",
      " [0.5097934  0.5097934  0.5097934  0.5097934  0.5097934 ]\n",
      " [0.48306414 0.48306414 0.48306414 0.48306414 0.48306414]\n",
      " [0.56395537 0.56395537 0.56395537 0.56395537 0.5639555 ]\n",
      " [0.5492917  0.5492917  0.5492917  0.5492917  0.5492918 ]\n",
      " [0.6431801  0.6431801  0.6431801  0.6431801  0.64318013]\n",
      " [0.4638112  0.4638112  0.4638112  0.4638112  0.4638112 ]\n",
      " [0.6235436  0.6235436  0.6235436  0.6235436  0.6235437 ]\n",
      " [0.5601643  0.5601643  0.5601643  0.5601643  0.5601643 ]\n",
      " [0.53675485 0.53675485 0.53675485 0.53675485 0.53675485]\n",
      " [0.5883428  0.5883428  0.5883428  0.5883428  0.58834285]\n",
      " [0.56237257 0.56237257 0.56237257 0.56237257 0.5623726 ]\n",
      " [0.62784153 0.62784153 0.62784153 0.62784153 0.6278416 ]\n",
      " [0.6023937  0.6023937  0.6023937  0.6023937  0.60239375]\n",
      " [0.6524615  0.6524615  0.6524615  0.6524615  0.6524616 ]\n",
      " [0.48600087 0.48600087 0.48600087 0.48600087 0.48600087]\n",
      " [0.63158137 0.63158137 0.63158137 0.63158137 0.6315814 ]\n",
      " [0.69775796 0.69775796 0.69775796 0.69775796 0.69775796]\n",
      " [0.63266885 0.63266885 0.63266885 0.63266885 0.6326689 ]\n",
      " [0.7042819  0.7042819  0.7042819  0.7042819  0.7042819 ]\n",
      " [0.6832262  0.6832262  0.6832262  0.6832262  0.68322635]\n",
      " [0.6883467  0.6883467  0.6883467  0.6883467  0.6883468 ]\n",
      " [0.6902434  0.6902434  0.6902434  0.6902434  0.6902435 ]\n",
      " [0.62740856 0.62740856 0.62740856 0.62740856 0.6274086 ]\n",
      " [0.6972567  0.6972567  0.6972567  0.6972567  0.69725674]\n",
      " [0.71641815 0.71641815 0.71641815 0.71641815 0.7164182 ]\n",
      " [0.79118437 0.79118437 0.79118437 0.79118437 0.7911845 ]\n",
      " [0.57152504 0.57152504 0.57152504 0.57152504 0.5715251 ]\n",
      " [0.75560147 0.75560147 0.75560147 0.75560147 0.7556015 ]\n",
      " [0.7047626  0.7047626  0.7047626  0.7047626  0.7047627 ]\n",
      " [0.71261746 0.71261746 0.71261746 0.71261746 0.7126176 ]\n",
      " [0.8115866  0.8115866  0.8115866  0.8115866  0.8115867 ]\n",
      " [0.73775434 0.73775434 0.73775434 0.73775434 0.7377544 ]]\n",
      "\n",
      "Output prediksi output layer : \n",
      "------------------------------\n",
      "[[ 0.14081454]\n",
      " [ 0.13088655]\n",
      " [-0.03187102]\n",
      " [ 0.36013544]\n",
      " [ 0.18170476]\n",
      " [ 0.11922967]\n",
      " [ 0.30829942]\n",
      " [ 0.27402544]\n",
      " [ 0.49347377]\n",
      " [ 0.07422912]\n",
      " [ 0.44757676]\n",
      " [ 0.29943824]\n",
      " [ 0.2447226 ]\n",
      " [ 0.36530077]\n",
      " [ 0.30459976]\n",
      " [ 0.4576224 ]\n",
      " [ 0.39814234]\n",
      " [ 0.5151676 ]\n",
      " [ 0.12609375]\n",
      " [ 0.4663638 ]\n",
      " [ 0.62104034]\n",
      " [ 0.46890545]\n",
      " [ 0.636289  ]\n",
      " [ 0.5870749 ]\n",
      " [ 0.59904313]\n",
      " [ 0.6034764 ]\n",
      " [ 0.45661044]\n",
      " [ 0.61986864]\n",
      " [ 0.6646553 ]\n",
      " [ 0.8394089 ]\n",
      " [ 0.3259921 ]\n",
      " [ 0.7562398 ]\n",
      " [ 0.6374124 ]\n",
      " [ 0.65577185]\n",
      " [ 0.8870957 ]\n",
      " [ 0.7145252 ]]\n",
      "\n",
      "Output bobot dari hidden layer ke output layer : \n",
      "------------------------------\n",
      "[[0.46746925 0.46746925 0.46746925 0.46746925 0.46746957]]\n",
      "\n",
      "Output bobot dari input layer ke hidden layer : \n",
      "------------------------------\n",
      "[[0.26466805 0.12170959 0.21901634 0.15812743 0.30868745 0.29767138\n",
      "  0.19896565 0.17645845 0.25628754 0.10525006 0.01847245]\n",
      " [0.26466805 0.12170959 0.21901634 0.15812743 0.30868745 0.29767138\n",
      "  0.19896565 0.17645845 0.25628754 0.10525006 0.01847245]\n",
      " [0.26466805 0.12170959 0.21901634 0.15812743 0.30868745 0.29767138\n",
      "  0.19896565 0.17645845 0.25628754 0.10525006 0.01847245]\n",
      " [0.26466805 0.12170959 0.21901634 0.15812743 0.30868745 0.29767138\n",
      "  0.19896565 0.17645845 0.25628754 0.10525006 0.01847245]\n",
      " [0.2646681  0.12170964 0.2190164  0.15812746 0.3086877  0.2976714\n",
      "  0.19896579 0.17645845 0.2562876  0.10525012 0.01847241]]\n",
      "\n",
      "Output bias dari hidden layer ke output layer : \n",
      "------------------------------\n",
      "[-1.0098554]\n",
      "\n",
      "Output bias dari input layer ke hidden layer : \n",
      "------------------------------\n",
      "[-0.48101702 -0.48101702 -0.48101702 -0.48101702 -0.48101702]\n",
      "------------------------------\n",
      "\n",
      "------------------------------\n",
      "AKURASI : 99.67795714735985%\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Call function \n",
    "v, w, b_k, b_j = train_model(TRAIN_ITER, w, v, b_j, b_k, y, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "svN4Vgbb_fNE",
    "outputId": "713539c3-8819-4c6c-f3bf-c85f16eb4f73"
   },
   "outputs": [],
   "source": [
    "# Test Model\n",
    "def test(x_test, y_test, w, v,  b_j, b_k):\n",
    "    z_prediction = np.dot(x_test, np.transpose(w)) + b_j\n",
    "    f_z_prediction = f_activation(z_prediction)\n",
    "    \n",
    "    y_prediction = np.dot(f_z_prediction, np.transpose(v)) + b_k\n",
    "    f_y_prediction =  f_activation_output(y_prediction)\n",
    "    \n",
    "    rms = rmse(f_y_prediction, y_test)\n",
    "    \n",
    "    print('-'*30)\n",
    "    print('TESTING MODEL')\n",
    "    print('-'*30)\n",
    "    print('\\nOutput prediksi hidden layer : ')\n",
    "    print('-'*30)\n",
    "    print(z_prediction)\n",
    "    print('\\nOutput prediksi output layer : ')\n",
    "    print('-'*30)\n",
    "    print(y_prediction)\n",
    "    print('\\nOutput bobot dari hidden layer ke output layer : ')\n",
    "    print('-'*30)\n",
    "    print(v)\n",
    "    print('\\nOutput bobot dari input layer ke hidden layer : ')\n",
    "    print('-'*30)\n",
    "    print(w)\n",
    "    print('\\nOutput bias dari hidden layer ke output layer : ')\n",
    "    print('-'*30)\n",
    "    print(b_k)\n",
    "    print('\\nOutput bias dari input layer ke hidden layer : ')\n",
    "    print('-'*30)\n",
    "    print(b_j)\n",
    "    print('-'*30)\n",
    "    print()\n",
    "    print('-'*30)\n",
    "    print('AKURASI : ' + str(rms) + '%')\n",
    "    print('-'*30)\n",
    "    return f_y_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "TESTING MODEL\n",
      "------------------------------\n",
      "\n",
      "Output prediksi hidden layer : \n",
      "------------------------------\n",
      "[[-0.03081557 -0.03081557 -0.03081557 -0.03081557 -0.03081542]\n",
      " [-0.04780594 -0.04780594 -0.04780594 -0.04780594 -0.04780573]\n",
      " [-0.32928005 -0.32928005 -0.32928005 -0.32928005 -0.32928002]\n",
      " [ 0.34799007  0.34799007  0.34799007  0.34799007  0.34799036]\n",
      " [ 0.03917453  0.03917453  0.03917453  0.03917453  0.03917471]\n",
      " [-0.06777278 -0.06777278 -0.06777278 -0.06777278 -0.06777263]\n",
      " [ 0.2572253   0.2572253   0.2572253   0.2572253   0.2572256 ]\n",
      " [ 0.19780019  0.19780019  0.19780019  0.19780019  0.19780043]\n",
      " [ 0.5891794   0.5891794   0.5891794   0.5891794   0.58917975]\n",
      " [-0.145008   -0.145008   -0.145008   -0.145008   -0.14500788]\n",
      " [ 0.50461185  0.50461185  0.50461185  0.50461185  0.5046121 ]\n",
      " [ 0.24182263  0.24182263  0.24182263  0.24182263  0.24182287]\n",
      " [ 0.14728013  0.14728013  0.14728013  0.14728013  0.1472803 ]\n",
      " [ 0.35711357  0.35711357  0.35711357  0.35711357  0.3571138 ]\n",
      " [ 0.25078967  0.25078967  0.25078967  0.25078967  0.25078997]\n",
      " [ 0.5229497   0.5229497   0.5229497   0.5229497   0.52294993]\n",
      " [ 0.41543427  0.41543427  0.41543427  0.41543427  0.4154347 ]\n",
      " [ 0.62987375  0.62987375  0.62987375  0.62987375  0.6298742 ]\n",
      " [-0.05602667 -0.05602667 -0.05602667 -0.05602667 -0.05602652]\n",
      " [ 0.5389893   0.5389893   0.5389893   0.5389893   0.53898966]\n",
      " [ 0.8366258   0.8366258   0.8366258   0.8366258   0.8366262 ]\n",
      " [ 0.54366326  0.54366326  0.54366326  0.54366326  0.5436636 ]\n",
      " [ 0.8677547   0.8677547   0.8677547   0.8677547   0.8677553 ]\n",
      " [ 0.7686187   0.7686187   0.7686187   0.7686187   0.76861906]\n",
      " [ 0.79238844  0.79238844  0.79238844  0.79238844  0.7923889 ]\n",
      " [ 0.8012414   0.8012414   0.8012414   0.8012414   0.80124176]\n",
      " [ 0.5211034   0.5211034   0.5211034   0.5211034   0.5211036 ]\n",
      " [ 0.83425784  0.83425784  0.83425784  0.83425784  0.8342583 ]\n",
      " [ 0.9267577   0.9267577   0.9267577   0.9267577   0.92675793]\n",
      " [ 1.3320808   1.3320808   1.3320808   1.3320808   1.3320817 ]\n",
      " [ 0.28806087  0.28806087  0.28806087  0.28806087  0.2880611 ]\n",
      " [ 1.1286982   1.1286982   1.1286982   1.1286982   1.1286986 ]\n",
      " [ 0.87007105  0.87007105  0.87007105  0.87007105  0.87007153]\n",
      " [ 0.9081205   0.9081205   0.9081205   0.9081205   0.908121  ]\n",
      " [ 1.4603437   1.4603437   1.4603437   1.4603437   1.4603442 ]\n",
      " [ 1.0343229   1.0343229   1.0343229   1.0343229   1.0343233 ]]\n",
      "\n",
      "Output prediksi output layer : \n",
      "------------------------------\n",
      "[[ 0.14081264]\n",
      " [ 0.13088858]\n",
      " [-0.03187263]\n",
      " [ 0.36013377]\n",
      " [ 0.18170607]\n",
      " [ 0.11923087]\n",
      " [ 0.30830073]\n",
      " [ 0.2740245 ]\n",
      " [ 0.49347186]\n",
      " [ 0.07423258]\n",
      " [ 0.44757974]\n",
      " [ 0.29943907]\n",
      " [ 0.24472392]\n",
      " [ 0.36530268]\n",
      " [ 0.30460024]\n",
      " [ 0.457618  ]\n",
      " [ 0.39813924]\n",
      " [ 0.51517177]\n",
      " [ 0.12608802]\n",
      " [ 0.46635962]\n",
      " [ 0.62103796]\n",
      " [ 0.46890008]\n",
      " [ 0.6362871 ]\n",
      " [ 0.5870712 ]\n",
      " [ 0.5990429 ]\n",
      " [ 0.6034746 ]\n",
      " [ 0.45660937]\n",
      " [ 0.61986995]\n",
      " [ 0.6646595 ]\n",
      " [ 0.83941746]\n",
      " [ 0.32598805]\n",
      " [ 0.75624037]\n",
      " [ 0.6374141 ]\n",
      " [ 0.6557739 ]\n",
      " [ 0.88710046]\n",
      " [ 0.7145294 ]]\n",
      "\n",
      "Output bobot dari hidden layer ke output layer : \n",
      "------------------------------\n",
      "[[0.46746925 0.46746925 0.46746925 0.46746925 0.46746957]]\n",
      "\n",
      "Output bobot dari input layer ke hidden layer : \n",
      "------------------------------\n",
      "[[0.26466805 0.12170959 0.21901634 0.15812743 0.30868745 0.29767138\n",
      "  0.19896565 0.17645845 0.25628754 0.10525006 0.01847245]\n",
      " [0.26466805 0.12170959 0.21901634 0.15812743 0.30868745 0.29767138\n",
      "  0.19896565 0.17645845 0.25628754 0.10525006 0.01847245]\n",
      " [0.26466805 0.12170959 0.21901634 0.15812743 0.30868745 0.29767138\n",
      "  0.19896565 0.17645845 0.25628754 0.10525006 0.01847245]\n",
      " [0.26466805 0.12170959 0.21901634 0.15812743 0.30868745 0.29767138\n",
      "  0.19896565 0.17645845 0.25628754 0.10525006 0.01847245]\n",
      " [0.2646681  0.12170964 0.2190164  0.15812746 0.3086877  0.2976714\n",
      "  0.19896579 0.17645845 0.2562876  0.10525012 0.01847241]]\n",
      "\n",
      "Output bias dari hidden layer ke output layer : \n",
      "------------------------------\n",
      "[-1.0098554]\n",
      "\n",
      "Output bias dari input layer ke hidden layer : \n",
      "------------------------------\n",
      "[-0.48101702 -0.48101702 -0.48101702 -0.48101702 -0.48101702]\n",
      "------------------------------\n",
      "\n",
      "------------------------------\n",
      "AKURASI : 99.67795649170876%\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Call function\n",
    "y_prediction_test = np.zeros(len(x), np.float32)\n",
    "y_prediction_test = test(x, y, w, v,  b_j, b_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DaoqXNOt_fNF",
    "outputId": "76d6d067-1753-47b0-fd93-4dca66d50e8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1286989.5, 1282481.2, 1208543.1, 1386621.4, 1305566.4, 1277185.5, 1363075.0, 1347504.2, 1447193.5, 1256743.9, 1426345.9, 1359049.4, 1334193.8, 1388969.5, 1361394.0, 1430906.0, 1403886.2, 1457051.1, 1280300.5, 1434877.0, 1505143.4, 1436031.1, 1512070.8, 1489713.2, 1495151.6, 1497164.9, 1430447.8, 1504612.8, 1524959.5, 1604347.5, 1371109.9, 1566562.4, 1512582.6, 1520923.0, 1626008.8, 1547614.1]\n"
     ]
    }
   ],
   "source": [
    "# Denormalize prediction\n",
    "j = data['J']\n",
    "denorm = []\n",
    "for i in range(len(data)):\n",
    "    y[i] = y_prediction_test[i] * (j.max(axis=0) - j.min(axis=0)) + j.min(axis=0) \n",
    "    denorm.append(y[i])\n",
    "print(denorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_b = {\n",
    "    'Januari' : 1,\n",
    "    'Februari' : 2,\n",
    "    'Maret' : 3,\n",
    "    'April' : 4,\n",
    "    'Mei' : 5,\n",
    "    'Juni' : 6,\n",
    "    'Juli' : 7,\n",
    "    'Agustus' : 8,\n",
    "    'September' : 9,\n",
    "    'Oktober' : 10,\n",
    "    'November' : 11,\n",
    "    'Desember' : 12\n",
    "}\n",
    "\n",
    "# encoding from string to binary\n",
    "data['B'] = data['B'].apply(lambda x : dict_b[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2017-1', '2017-2', '2017-3', '2017-4', '2017-5', '2017-6', '2017-7', '2017-8', '2017-9', '2017-10', '2017-11', '2017-12', '2018-1', '2018-2', '2018-3', '2018-4', '2018-5', '2018-6', '2018-7', '2018-8', '2018-9', '2018-10', '2018-11', '2018-12', '2019-1', '2019-2', '2019-3', '2019-4', '2019-5', '2019-6', '2019-7', '2019-8', '2019-9', '2019-10', '2019-11', '2019-12']\n"
     ]
    }
   ],
   "source": [
    "df1 = data.loc[:,['T']].values\n",
    "df2 = data.loc[:,['B']].values\n",
    "\n",
    "df11 = df1.astype(str)\n",
    "df22 = df2.astype(str)\n",
    "dff1 = df11.flatten()\n",
    "dff2 = df22.flatten()\n",
    "dff= []\n",
    "for i in range(len(dff1)):\n",
    "    dffi = dff1[i] +\"-\"+ dff2[i]\n",
    "    dff.append(dffi)\n",
    "print(dff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "CNSU4JWN_fNG",
    "outputId": "edb6b7be-fa49-4b8a-f9fc-bc0c7df6e8b4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='/home/systemcommand/anggy/app/data/result.csv' target='_blank'>/home/systemcommand/anggy/app/data/result.csv</a><br>"
      ],
      "text/plain": [
       "/home/systemcommand/anggy/app/data/result.csv"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Output prediction File\n",
    "submission = pd.DataFrame({\n",
    "    \"Time\" : dff,\n",
    "    \"Truth\" : data['J'],\n",
    "    \"Prediction\": denorm\n",
    "    })\n",
    "\n",
    "submission.head(5)\n",
    "\n",
    "# Export it in a 'Comma Separated Values' (CSV) file\n",
    "submission.to_csv(r'/home/systemcommand/anggy/app/data/result.csv', index=False)\n",
    "# Creating a link to download the .csv file we created\n",
    "from IPython.display import FileLink\n",
    "FileLink(r'/home/systemcommand/anggy/app/data/result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 523
    },
    "id": "dzYCYW0z_fNI",
    "outputId": "82a40adf-c9b7-443d-be86-a36ece51664c"
   },
   "outputs": [],
   "source": [
    "# # Plot prediction and truth data\n",
    "# dataa = pd.read_csv('/home/systemcommand/anggy/app/data/result.csv')\n",
    "\n",
    "# x = dataa['Truth']\n",
    "# x2 = dataa['Prediction']\n",
    "# y = dataa['Bulan']\n",
    "\n",
    "# plt.figure(figsize=(16,8))\n",
    "# plt.scatter(y,x2,c='red')\n",
    "# plt.plot(y,x2,c='red', label=\"Prediction\")\n",
    "# plt.scatter(y,x,c='blue')\n",
    "# plt.plot(y,x,c='blue', label=\"Actual\")\n",
    "# plt.legend(loc='upper left', fontsize = 20)\n",
    "# plt.ylabel('prediction', fontsize = 15)\n",
    "# plt.xlabel('period', fontsize = 15)\n",
    "# plt.title('Perbandingan data aktual dan prediksi', fontsize = 20)\n",
    "# plt.xticks()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "5-15000.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
